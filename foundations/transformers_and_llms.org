#+TITLE: Transformers
#+AUTHOR: N. Arun Varma
#+bibliography: /Users/arun/bibliography/bibliography.bib
#+ATTR_HTML: align middle

* Introduction
The transformer architecture introduced in the paper by
[cite:@vaswani2017attention] has become the de-facto choice for
language modeling. The transformer model is based on the central idea
of */self-attention/* which can be loosely described as computing
representation for a given token by composing the representations of
tokens in its neighborhood weighted by their relevance. The
effectiveness of the transformer architecture stems from the fact that
unlike earlier models which were based on recurrent neural networks,
the transformer architecture lends itself to GPU parallelism and can
be trained efficiently (compared to RNNs) on extremely large corpora.

The transformer based models have been amazingly successful over a
wide range of NLP tasks including question answering, text
summarization, sentiment detection, translation, creative writing etc.
While the successes are well recognized there is also a sense of
concern about the resources required to train and infer using these
models, their propensity to hallucinate, privacy concerns, copyright
infringement and perhaps most importantly the lack of guarantees in
settings that require responses to be grounded in facts and domain
knowledge. Some of the popular Large Language Models (LLMs) based on
the transformer architecture are show in [[llms][Table 1.]] (source: Wikipedia).

#+CAPTION: Popular LLMs
#+NAME: llms
#+ATTR_HTML: align right
| Name         | Parameters[fn::in billions]  | Organization | Open Source | Remarks                                       |
|--------------+-----------------------+--------------+-------------+-----------------------------------------------|
| Bert         |                 0.340 | Google       | Yes         | Encoder-only model                            |
| Phi-2        |                   2.7 | Microsoft    | Yes         | Trained on text-book quality data.            |
| Mistral 7B   |                     7 | Mistral AI   | Yes         |                                               |
| T5           |                    11 | Google       | Yes         | Base model for many Google projects.          |
| BloombergGPT |                    50 | Bloomberg    | No          | Trained on financial data                     |
| Claude       |                    52 | Anthropic    | ?           | Fine tuned fore responsible chat behavior.    |
| Chinchilla   |                    70 | DeepMind     | No          | Reduced parameter model trained on more date. |
| LLama 2      |                    70 | Meta AI      | No          |                                               |
| GPT-3        |                   175 | OpenAI       | No          | The base model behind the popular ChatGPT.    |
| Llama 3      |                   405 | Meta AI      | No          |                                               |



* Self attention
Self attention is the central concept behind the Transformer
model. Informally self-attention is a mechanism for computing the
embedding of a token from the embeddings of the tokens in the
neighborhood while weighing them based on relevance.

More formally consider a set of tokens $x_1,\ldots,x_n$ and their
embeddings $v_1,\ldots,v_n$. At its core self attention is the
following computation that yields a new embedding $a_i$ for token
$x_i$

\begin{equation}
a_i = \sum_{j} \alpha_{ij}v_j
\end{equation}

Here $\alpha_{ij}$ are the relevance weights (i.e., relevance of token
$x_j$ in the computation of the embedding for $x_i$). The relevance
weights are naturally computed using the dot product which measures
the similarity of the two tokens. However, instead of using the
existing embeddings $V$ to compute relevance weights, the transformer
model uses two distinct sets of embeddings that indicate the roles
being performed by the tokens in the computation:
1. Query embeddings: the embedding of $x_i$ in its role as the token
   for which a new embedding is being computed.
2. Key embeddings: the embedding of $x_j$ in its role as the token
   being compared to $x_i$ in the computation of relevance weights.

For a single token with input embedding $x_i$, its query, key and
value embeddings are computed by using projection matrices $W^Q, W^K$
and $W^V$.

\begin{align}
q_i &= x_i  W^Q\\
k_i &= x_i  W^K\\
v_i &= x_i  W^V
\end{align}

The relevance weights $\alpha_{ij}$ are computed by first taking the
dot product $q_i k_j^T$ and normalizing it using a softmax.

\begin{equation}
\alpha_{ij} = \frac{exp(q_i k_j^T)}{\sum_j exp(q_i k_j^T)}
\end{equation}

Finally the new embedding of $x_i$ denoted $a_i$ is computed as

\begin{equation}
a_i = \sum_j \alpha_{ij}v_j
\end{equation}

Code snippets implementing the idea of self-attention is shown below
#+INCLUDE: "../src/transformer.py" src python :lines "38-42"
#+INCLUDE: "../src/transformer.py" src python :lines "48-50"

** Scaled self attention
The self attention used in the transformer model is a slightly
modified version of the self attention described above. In particular,
the relevance weights are "moderated" before being input to the
softmax function. This is done because the result of the dot product
attention can be very large or very small and when exponentiated can
give rise to extreme values where gradients may not be effectively
computed. Concretely the dot product attention is "scaled" by dividing
it by the square root of the number of dimensions of the embeddings
being used. The number of dimensions of the query and key embeddings
are same and let it be denoted $d_k$. The scaled dot product attention is

\begin{equation}
\frac{q_i k_j^T}{\sqrt{d_k}}
\end{equation}

In terms of code it is
#+INCLUDE: "../src/transformer.py" src python :lines "42-43"

** Backward a.k.a "causal" self attention
When transformers are used for "auto-regressive" or "causal" language
modeling[fn::contrived way of saying modeling for next word
prediction. Causality is a well defined term in AI but has been
bastardized by the DL community], the embedding of a token is computed
only from the tokens that precede it in the context. In order to
preserve the GPU parallelism it is common to replace the relevance
weights of succeeding tokens with $- \infty$ and computing the
attention probabilities as usual.

** Parallel computation
In practice, the self attention computations of all tokens in the
context window is performed in a batched fashion by leveraging
efficient matrix multiplication implemented by GPUs. Consider
- $X$ to be an $N \times d$ matrix that contains the input embeddings
  of $N$ tokens in the context window.
- $W^Q$ is the matrix to project input embeddings to query
  embeddings. It is shaped $d \times d_k$
- $W^K$ is the matrix to project input embeddings to key
  embeddings. It is shaped $d \times d_k$
- $W^V$ is the matrix to project input embeddings to value
  embeddings. It is shaped $d \times d_v$.

We can describe the parallel computation of scaled dot product
attention as follows
\begin{align}
Q &= X W^Q\\
K &= X W^K\\
V &= X W^V\\
\alpha &= softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)\\
A &= \alpha V
\end{align}

The above code snippets are general enough that they can be used for
batched computation without any changes. The complete code that
implements scaled dot product attention in shown below.

#+INCLUDE: "../src/transformer.py" src python :lines "9-50"

* Multihead attention
Multihead attention is the generalization of the self-attention
concept that computes multiple representations/embeddings for each
token. The motivation behind multihead attention is that a token might
be related to nearby tokens in multiple ways and a single
self-attention mechanism may not capture all the details. In this way
one can compose an embedding from nearby embeddings by using
grammatical number, coreference, word sense etc. The final step of the
multihead attention computation involves projecting the concatenated
embeddings back to the embeddings of the same dimensionality as the
input embeddings.

More formally let $W_i^Q, W_i^K$ and $W_i^V$ denote the query, key and
value projection matrices for "head" $i$. Given a matrix of input
embeddings $X$, the self-attention computation for each head proceeds
as before yielding $A_i$. The final output of the multihead attention
module involves concatenating the $A_i$ and projecting back to input
space by using a matrix shaped $hd_v \times d$ where $h$ is the number
of heads and $d_v$ is the dimensionality of value embeddings as
before. Let $\oplus$ denote concatenation of matrices, then the final
output of multihead attention is computed as

\begin{equation}
O = (head_1 \oplus head_2 \cdots \oplus head_h)W^O
\end{equation}

The implementation of multihead attention in code is shown below

#+INCLUDE: "../src/transformer.py" src python :lines "52-92"

** Residual view of the transformer
The residual stream view of the transformer presents a picture of
input embeddings passing through the transformer to the output of the
transformer.  Along the way the output of multiheaded attention and
the output of the feedforward network get added to the stream. Further
only multihead attention looks at other inputs in the neighborhood,
whereas the feedforward network looks only at the token for the
current residual stream.

The multihead transformers can be viewed as moving information from
other residual streams into the current one.

* Transformer blocks
A transformer block consists of multihead attention together with a
feedforward network, residual connections and normalizing layers.

** Input embeddings
The input to the transformer model is a matrix of embeddings. More
specifically its a matrix of shape N x d where N is the number of tokens
in the context and d is the embedding dimensionality. The embeddings
are learned representations. In order to capture the positions of the
tokens positional embeddings are used.

The specific positional embeddings used in the transformer paper are
based on sine and cosine functions. The idea is to input the word
position as input to the sine and cosine functions interpreting it as
the angle in radians. A single sine function or cosine function or
even a pair will not suffice because multiple positions get mapped to
the same result. Therefore the embedding uses many dimensions. The
even dimensions get their value from sine function and the odd
dimensions get their value from cosine functions. Further the
wavelengths of the various sine and cosine functions is
changed. Specifically the wavelengths of leftmost dimensions is
shorter.

** Language modeling head
Recall that the construction of the transformer model maps input
embeddings to output embeddings. The mapping has the property that
output embedding for the last token represents the next token in the
sequence. Given an embedding representing the next token in the
sequence we need additional machinery to generate the actual token.

Note that the output embedding is not one-hot encoded
vector. Therefore when we multiply it with an unembedding matrix we
get the weights of each possible next token. These weights can be
converted to a probability distribution using a softmax
function. Finally we can use the probability distribution to sample
the next token.

The additional machinery which does this is called the language
modeling head. Typically the unembedding matrix not a separate tunable
parameter.  Instead it is just the transpose of the embedding matrix.


* Why are transformers good as Language Models?
Most of the tasks that the transformers are used for can be posed as
conditional generation tasks. Language models are naturally suited to
these tasks because they encode joint distribution of
sentences. Moreover transformers have the advantage of having very
long context windows for conditional generation which enables them to
condition on the priming context as well as their own generated
output.

The insight of large language modeling is that many practical
applications can be cast as conditional generation. Consider the task
of sentiment analysis.  At a first glance, it looks like a
classification problem. But we can use the following prompt to recast
the classification problem to a conditional generation problem.

Given the following piece of text

"lore ipsum blah blah blah..".

The sentiment expressed in the text is ?


It is quite wierd to think about it, but it looks like any conceivable
problem can be cast as a conditional generation problem because any
problem can be posed as a question in natural language and large
language models are excellent at modeling the joint distribution of
natural language sentences.

Heck we can even pose complex questions like graph reachability by
simply describing the graph in natural language and asking if there
exists a path between a pair of nodes? Instead of algorithmically
solving the question, the large language model uses autoregressive
modeling to solve the question.

For another interesting example, consider the task of text
summarization.  Here the key idea is to use a natural language prompt
to guide the large language model to produce a summary.  It is common
practice in written communication to use the abbreviation tl;dr to
summarize long emails/notes. So prompting the large language model in
the following way could make it produce a summary

"lore ipsum blah blah blah...."

tl;dr;

To reiterate large language models succeed at these tasks because
their large context windows allow them to pay attention to not just
the prompt, but also their generated text throughout the process.

The completion can be done in several ways. One could use greedy
sampling in which we always choose the most probable next word. This
is a locally optimal solution, but may not result in the best possible
completion overall. The other choices are beam sampling where we
maintain a pool of best candidate solution and extend them while
keeping the size of the pool constant. Finally we have probabilistic
sampling in which completions are generated according to the joint
distribution.

The field of deep learning is very good at bastardizing terms. The
conditional generation is also called as autoregressive generation and
causal language modeling. It should be noted that terms
"autoregressive" and "causal" are precise technical terms with long
histories.


There are some variations of random sampling
- Top k sampling. Keep only the top k words by probability,
  renormalize the distribution and sample from it.
- Top p sampling/nucleus sampling. Keep the highest probability words
  that together constitute probability mass of p. Renormalize and
  sample from the distribution
- Temperature sampling. Divide the logits by temperature. When
  temperature is high (i.e., close to 1), it returns the original
  distribution and hence "samples from a wide range". When temperature
  is close to zero the division causes larger values to become more
  extreme. So the resulting distribution places most of the
  probability mass on a few choices.


* Training a transformer to be a language model.
