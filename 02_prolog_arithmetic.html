<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2024-06-26 Wed 18:10 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Arithmetic Reasoning with LLM: Prolog Generation &amp; Permutation (summary)</title>
<meta name="author" content="N. Arun Varma" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
</head>
<body>
<div id="content" class="content">
<h1 class="title">Arithmetic Reasoning with LLM: Prolog Generation &amp; Permutation (summary)</h1>
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org1beb8f8">1. Motivation</a></li>
<li><a href="#orgdd78c51">2. Logic programming and Prolog</a></li>
<li><a href="#org7ad8dc6">3. GSM8K-Prolog dataset</a></li>
<li><a href="#orgc0b3fa1">4. Prolog permutation</a></li>
<li><a href="#orgcd39359">5. Experiments</a>
<ul>
<li><a href="#org35fc411">5.1. Permuation ratio</a></li>
<li><a href="#org6d56ea3">5.2. Mismatch between loss and accuracy</a></li>
</ul>
</li>
<li><a href="#org93c3b8f">6. Conclusion</a></li>
<li><a href="#orgd3eb278">7. Bibliography</a></li>
</ul>
</div>
</div>

<div id="outline-container-org1beb8f8" class="outline-2">
<h2 id="org1beb8f8"><span class="section-number-2">1.</span> Motivation</h2>
<div class="outline-text-2" id="text-1">
<p>
The authors claim (like the authors of PAL paper) that LMs have
difficulty solving arithmetic (and reasoning) tasks. However, they
depart from the premise of the PAL work that LMs are good at
decomposing a problem into subtasks and only need help from an
interpreter to correctly execute the decomposition steps. The authors
contend that techniques like COT prompting and PAL are prone to
cascading of errors due to the sequential nature of the decomposition
steps. They hypothesize that solving the problem in a non sequential
fashion, by having the LM extract facts and rules and delegating the
inference to a Prolog interpreter should improve the performance.
</p>

<p>
To restate the motivation of the paper, existing techniques like COT
and PAL rely on a sequence of reasoning steps/programming statements
to solve a problem which might lead to cascading of errors. The use of
a prolog interpreter lifts the constraint of sequential problem
decomposition and can improve the performance of LMs.
</p>
</div>
</div>

<div id="outline-container-orgdd78c51" class="outline-2">
<h2 id="orgdd78c51"><span class="section-number-2">2.</span> Logic programming and Prolog</h2>
<div class="outline-text-2" id="text-2">
<p>
Briefly a logic program consists of a set of facts and rules. In the example below
</p>
<div class="org-src-container">
<pre class="src src-prolog">% Weng earns $12 an hour for babysitting. Yesterday, she just did 50
% minutes of babysitting. How much did she earn?

hourly_rate(weng, 12).
mins_worked(weng, 50).

hours_worked(Person, Hours) :-
    mins_worked(Person, Minutes),
    Hours is Minutes / 60.

earnings(Person, Earings) :-
    hourly_rate(Person, Rate),
    hours_worked(Person, Hours),
    Earnings is Rate * Hours.
    
</pre>
</div>
<p>
The facts are <code>hourly_rate(weng, 12)</code> and <code>mins_worked(weng, 50)</code>. The
rules are the ones describing how <code>hours_worked</code> is related to
<code>mins_worked</code> and how <code>earnings</code> is related to <code>hourly_rate</code> and
<code>hours_worked</code>. Each logic program has associated semantics (i.e.,
what is true according to the program). For programs without negation,
the semantics is given by the <i>least herbrand model</i>. Prolog
interpreter employes sound (returns answers that are consistent with
the semantics) but incomplete refutation search procedure to answer
queries. The ordering of the rules and facts does not impact the
semantics of the program. The ordering of the subgoals in the body of
a rule also does not affect the semantics. Strictly speaking the
ordering of the rules does impact the results returned by the Prolog
interpreter because it uses the rules in the order in which they are
specified. But this technicality was glossed over by the authors since
they must have dealt with very trivial programs where the ordering of
the rules was immaterial.
</p>
</div>
</div>

<div id="outline-container-org7ad8dc6" class="outline-2">
<h2 id="org7ad8dc6"><span class="section-number-2">3.</span> GSM8K-Prolog dataset</h2>
<div class="outline-text-2" id="text-3">
<p>
The authors contribute a dataset of Prolog solutions to GSM8K
problems. It is to be noted that these are not handcrafted
solutions. They were generated by GPT-4 and underwent basic sanity
checking. We can in no way assume that these are idiomatic prolog
solutions. The convoluted algorithm used to generate the
<i>GSM8K-Prolog</i> dataset can be summarized as follows: Bootstrap the
generation process by using 10 manually crafted prolog solutions and
COT reasoning chains for the problems. In other words give COT
reasoning chains and corresponding prolog code for 10 problems and
prompt the LM to generate reasoning steps and code for the next
problem. Repeat the process this time by using 20 examples &#x2013; 10 of
which are manually selected clean and correct codes and the rest are
randomly sampled. The dataset generation ends by manual correction of
wrong prolog programs.
</p>
</div>
</div>

<div id="outline-container-orgc0b3fa1" class="outline-2">
<h2 id="orgc0b3fa1"><span class="section-number-2">4.</span> Prolog permutation</h2>
<div class="outline-text-2" id="text-4">
<p>
As mentioned earlier, the semantics of a prolog program does not
change under permutation of rules, facts and subgoals within a
rule. In contrast, language models are autoregressive in nature and
estimate the next token based on the tokens in the context. In order
to model the permutation invariance, the authors used ideas from XLNet
to provide the trained models with a degree of autoencoding property.
</p>
</div>
</div>

<div id="outline-container-orgcd39359" class="outline-2">
<h2 id="orgcd39359"><span class="section-number-2">5.</span> Experiments</h2>
<div class="outline-text-2" id="text-5">
<p>
The authors compared three approaches &#x2013; standard COT, finetuning on
GSM8K-Prolog and finetuning on GSM8K-Prolog augmented with permuted
prolog samples. The PAL approach is conspicuously absent from the
evaluation.
</p>

<p>
The training and evaluation can be briefly described as follows: The
training data &#x2013; whether it is GSM8K-Prolog or the augmented version
is split into train/test and validation parts. The models are
finetuned on the training dataset using LoRA technique with a variety
of hyperparameters. For evaluation beam search is used to generate
Prolog code and the resulting code is passed to a prolog
interpreter. The evaluation metric used is accuracy. The LMs used were
7B versions of Llama2, CodeLlama and Mistral. It is interesting to
note that the authors did not use the LMs used in the PAL paper. Apart
from training and evaluating on GSM8K, the authors also evaluated the
performance of the models on GSM8K-Hard which replaces the numbers in
the problems with large random numbers. The performance is shown in
Figure 1. Across all the models and datasets we observe that
generating Prolog code performs better than COT reasoning and using
permuted samples gives a further boost to the performance. Mistral is
the best performing model without generating Prolog code, but even
this model gets a significant improvement in the performance through
the use of Prolog generation. The authors claim that CodeLlama gets
the best improvement in performance because it is pre-trained on code
samples and is thus better at generating programs. However Mistral is
pretrained on code samples.
</p>


<div id="org374de69" class="figure">
<p><img src="./images/prolog_arithmetic/results.png" alt="results.png" width="700px" />
</p>
<p><span class="figure-number">Figure 1: </span>Accuracy of competing approaches (Yang, Xiaocheng and Chen, Bingsen and Tam, Yik-Cheung, 2024)</p>
</div>
</div>

<div id="outline-container-org35fc411" class="outline-3">
<h3 id="org35fc411"><span class="section-number-3">5.1.</span> Permuation ratio</h3>
<div class="outline-text-3" id="text-5-1">
<p>
One salient point from the experiments is that the number of permuted
samples added per instance is very low. Only one or two permuted
examples were added. It is not clear why higher permutation ratios
were not explored. A surprising result is that Mistral experienced a
drop in performance when going from permutation ratio of 1:1 to
1:2. No satisfactory explation is provided and authors guess that
Mistral may have high prolog generation capacity and therefore does
not benefit from increased permuation ratio. But why should it suffer
a drop in performance?
</p>
</div>
</div>

<div id="outline-container-org6d56ea3" class="outline-3">
<h3 id="org6d56ea3"><span class="section-number-3">5.2.</span> Mismatch between loss and accuracy</h3>
<div class="outline-text-3" id="text-5-2">
<p>
The authors reported that when validation loss and accuracy are
plotted, a divergence is observed between the two metrics. For Llama2
the validation loss drops and then increases, but the accuracy
increases and continues to remain at a high level. The authors
suggested that this should be exploited, but gave no insight into this
unexpected behavior. This is shown in Figure 2.
</p>

<p>
The authors also note that if validation samples are not carefully
split, then there would be soft data leakage into the training dataset
because the same problem can end up being part of the training dataset
and the validation dataset in permuted forms. 
</p>


<div id="org6cb7259" class="figure">
<p><img src="./images/prolog_arithmetic/divergence.png" alt="divergence.png" width="700px" />
</p>
<p><span class="figure-number">Figure 2: </span>Divergence of loss and accuracy (Yang, Xiaocheng and Chen, Bingsen and Tam, Yik-Cheung, 2024)</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org93c3b8f" class="outline-2">
<h2 id="org93c3b8f"><span class="section-number-2">6.</span> Conclusion</h2>
<div class="outline-text-2" id="text-6">
<p>
The authors explored an interesting approach to program aided language
models &#x2013; that of prolog generation and inference. They rightly note
that prolog code is invariant to certain permutations and therefore
can benefit through the addition of permuted samples to the training
dataset and use of techniques to enable autoencoding. However, the
work is hampered because of lack of thorough experimental evaluation
and analysis. The authors surprisingly overlook the PAL approach and
the models used in that work. Even though PAL paper used Python
interpreter, it would have been illuminating to compare the
performance of the corresponding LMs using Prolog code generation.
</p>
</div>
</div>

<div id="outline-container-orgd3eb278" class="outline-2">
<h2 id="orgd3eb278"><span class="section-number-2">7.</span> Bibliography</h2>
<div class="outline-text-2" id="text-7">
<p>
Yang, Xiaocheng and Chen, Bingsen and Tam, Yik-Cheung (2024). <i>Arithmetic Reasoning with LLM: Prolog Generation \&amp; Permutation</i>, arXiv preprint arXiv:2405.17893.</p>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2024-06-18 Tue 00:00</p>
<p class="author">Author: N. Arun Varma</p>
<p class="date">Created: 2024-06-26 Wed 18:10</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>